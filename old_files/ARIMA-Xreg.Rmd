---
title: "R Notebook"
---
```{r}
library(astsa)
library(forecast)
library(fpp)
```

First we import our data -- NASDAQ daily Close price from 2010 to 2018.
```{r}
## Data Prepare
Nas_Int=data.frame(read.csv('NASDAQ.csv',header = TRUE))
Nas = ts(Nas_Int[,3][1:2264])
```

Let's see the movement of our data.
```{r}
plot(Nas, xlab='Time', ylab = 'NASDAQ')
```

Apparently, the NASDAQ daily close price has an ascending trend.




# Methods

## Arima: Autoregresive Integrating Moving Average

The fact of introducing ARIMA models comes from the assumption that we are not working with a non stationary dataset series. We say that time series datasets are stationary when their means, variance and autocovariance do not change during time. The mayority of economic time series are not stationary, but differencing them determined number times makes them stationary. With this previous operation we can apply ARIMA models to any stock price.

The construction of the models is made by iterative approaches using 4 steps:

-Identification: With the time dataset we try to incorporate a relevant research model. The objective is to find the best values reproducting the time set variable to forecast.

-Analysis and Differentiation: This step consists on studying the time set. In this study we incorporate different statistical tools like ACF and PACF tests, selecting the model parameters.

-Improving ARIMA model: We extract the determination coeficients and adjust the model, adding regressors to the model.

-Prediction: Once we have selected the best models, we can make a forecasting based on probabilistic future values.

First we conduct an ADF test for the close price set:
```{r}
print(adf.test(Nas))
```

The result of ADF test shows that our data is non-stationary, which is within our expectation.

After the ADF test we apply the ACF and PACF functions to the dataset.
Since our data has an ascending trend, we also apply the ACF and PACF function on differenced data.

```{r}
par(mfcol=c(3,2))
plot(Nas, xlab='Time', ylab = 'NASDAQ')
acf(Nas, main ="ACF - NASDAQ", lag.max = 20)
pacf(Nas, main ="PACF - NASDAQ", lag.max = 20)
plot(diff(Nas), xlab='Time', ylab = 'NASDAQ.DIFF')
acf(diff(Nas), main ="ACF - NASDAQ.DIFF", lag.max = 20)
pacf(diff(Nas), main ="PACF - NASDAQ.DIFF", lag.max = 20)
```

```{r}
###################################################################################
#################       ARIMA Models   （注明无seasonality）     ##################
###################################################################################
```


We use auto.arima function to seek the best model. 
We can see our model factors as below.
```{r}
## AUTO.ARIMA
ARIMA.Auto=auto.arima(Nas)
summary(ARIMA.Auto)
```

The result ARIMA model is ARIMA(2,1,2).



```{r}
# Residuals of ARIMA.Auto
par(mfrow = c(2,1))
res.auto = residuals(ARIMA.Auto)
plot(res.auto, main = 'Residuals of ARIMA.Auto')
acf(res.auto, main = 'ACF - Residuals of ARIMA.Auto')
```

As we can see from the graph, the residuals is almost a white noise. 

Let's see the Ljung-Box p-values of the model. For the Ljung-Box test we have that our null hypothesis is:
-H0: The dataset points are independently distributed.
With this null hypothesis, a significant p-value greater than 0.05 does not rejects the fact that the dataset points are not correlated.
```{r}
Box.test(res.auto, type = c("Box-Pierce","Ljung-Box"))
```

In this test we can see that the p-value is far from 0 so that our null hypothesis is not rejected.


Based on a purpose of further improve our model, we consider adding on a regressor -- INTEREST RATE, which is closely related to stock price.
```{r}
## Add a regressor -- interest rate
Int = ts(Nas_Int[,4][1:2264])
par(mfrow = c(2,1))
plot(diff(Nas), xlab='Time', ylab = 'NASDAQ.DIFF')
plot(Int, xlab='Time', ylab = 'Interest Rate')
```

From the plots above, we can see that the difference of NASDAQ index and the interest rate almost fluctuate at the same time.

Again, we use auto.arima function to gives us the better approach to the dataset, so we will not deep the analysis on finding model parameters here.
```{r}
ARIMA.Xreg=auto.arima(Nas,stepwise = FALSE, approx = FALSE, xreg = Int, seasonal=TRUE)
summary(ARIMA.Xreg)
```

The result model is ARIMA(4,1,1).


```{r}
# Residuals of ARIMA.Auto
par(mfrow = c(2,1))
res.auto = residuals(ARIMA.Xreg)
plot(res.xreg, main = 'Residuals of ARIMA.Xreg')
acf(res.xreg, main = 'ACF - Residuals of ARIMA.Xreg')
```

As we can see from the graph, the residuals is almost a white noise. 

We also do the Ljung-Box p-values of the model with regressor.
```{r}
Box.test(res.xreg, type = c("Box-Pierce","Ljung-Box"))
```

The p-value is far from 0 so that our null hypothesis is again not rejected, allowing us to continue our study with a solid motivation.


Which one is better, with or without regressor? We compare the AIC and BIC of two models.
```{r}
## AIC & BIC
AIC(ARIMA.Auto)
AIC(ARIMA.Xreg)
BIC(ARIMA.Auto)
BIC(ARIMA.Xreg)
```

The second model has both smaller AIC and BIC. So adding a regressor benefits the performance of our model.

```{r}
#sarima(Nas,2,1,2)
#sarima(Nas,4,1,1,xreg = Int)

# 以上两个p-value显示在lag8以后，全部都显著，意思就是当lag大于等于8的时候，序列都有相关性。
#（我们其实需要所有lag的p-value都大于0.05）这个原因不知道是不是整个序列是上升的。

# 如果要解决这个问题，那我们就必须要找到一个season，可是无论从数据里还是从实际意义里，
# 这个序列又找不到一个合适的season。就让人很难办.

# 实在不行就不要这个了 =.=
```


Having our ARIMA models applied and analysed we can plot the model prediction in a red line over the real train set stock close price in blue.

```{r}
## Training data fitting 
par(mfrow = c(2,1))
plot(Nas[1:2264],col="blue", lwd = 1.5,type = 'l', xlab = 'Time', ylab = 'NASDAQ', main = "Fitted ARIMA.Auto")
lines(fitted(ARIMA.Auto),col="red",lwd = 0.5,type = 'l')
plot(Nas[1:2264],col="blue", lwd = 1.5,type = 'l', xlab = 'Time', ylab = 'NASDAQ', main = "Fitted ARIMA.Xreg")
lines(fitted(ARIMA.Xreg),col="red",lwd = 0.5,type = 'l')
```

Both plots indicate that our models fit the actual data quite well. We need to remind that these are auto regressive models, so we are going to have very good past predictions. Now with the model fitted we can proceed to forecast our daily close price values to the future. We focus on forecasting the close Nasdaq index for the next year, 2019.


As we can see, we have a blue line that represents the mean of our prediction, a red line that represents the actual data in 2019. With the blue line explained we can see a darker and light darker areas, representing 80% and 95% confidence intervals respectively in lower and upper scenarios.
```{r}
## Forcasting
par(mfrow = c(2,1))
pred = forecast(ARIMA.Auto,h = 251)
plot(pred)
lines(c(2265:2515),Nas_Int[,3][2265:2515], col = 2)
pred = forecast(ARIMA.Xreg,h = 251, xreg = as.numeric(rep(FALSE,251)))
plot(pred)
lines(c(2265:2515),Nas_Int[,3][2265:2515], col = 2)
```

As we can see, both models indicate an upward trend. There is a downward trend at the end of the training data. The first model is affected by this and gives a lower prediction in 2019. While our second model overcomes this misleading information and gives a relevantly accurate prediction, much better than the first one.


We try to explain why the ARIMA model with a regressor can make better predictions.
```{r}
Int = ts(Nas_Int[,4][1:2264])
par(mfrow = c(2,1))
plot(Nas, xlab='Time', ylab = 'NASDAQ.DIFF')
plot(Int, xlab='Time', ylab = 'Interest Rate')
```

When Nasdaq index descended in late 2018, the interest rate get on an upward trend. We can assume that this wave of stock descending came from the increase of interest rate. As the interest rate would be adjusted in the near future --it's impossible to increase all the time -- the stock price would quickly get back to its way in increasing.


